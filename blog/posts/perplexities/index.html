
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://oobabooga.github.io/blog/posts/perplexities/">
      
      
        <link rel="prev" href="../gptq-awq-exl2-llamacpp/">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.6">
    
    
      
        <title>A direct comparison between llama.cpp, AutoGPTQ, ExLlama, and transformers perplexities - oobabooga blog</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.35e1ed30.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#a-direct-comparison-between-llamacpp-autogptq-exllama-and-transformers-perplexities" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="oobabooga blog" class="md-header__button md-logo" aria-label="oobabooga blog" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            oobabooga blog
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              A direct comparison between llama.cpp, AutoGPTQ, ExLlama, and transformers perplexities
            
          </span>
        </div>
      </div>
    </div>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="oobabooga blog" class="md-nav__button md-logo" aria-label="oobabooga blog" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    oobabooga blog
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Blog
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../tags/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tags
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Posts
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Posts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../gguf-vram-formula/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A formula that predicts GGUF VRAM usage from GPU layers and context length
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../gptq-awq-exl2-llamacpp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A detailed comparison between GPTQ, AWQ, EXL2, q4_K_M, q4_K_S, and load_in_4bit: perplexity, VRAM, speed, model size, and loading time.
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    A direct comparison between llama.cpp, AutoGPTQ, ExLlama, and transformers perplexities
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    A direct comparison between llama.cpp, AutoGPTQ, ExLlama, and transformers perplexities
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#how-i-did-it" class="md-nav__link">
    How I did it
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation-setup" class="md-nav__link">
    Evaluation setup
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    Results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    Key takeaways
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#how-i-did-it" class="md-nav__link">
    How I did it
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation-setup" class="md-nav__link">
    Evaluation setup
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    Results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    Key takeaways
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<div class="md-typeset">
    <div class="blogging-tags-grid">

        <a href="https://oobabooga.github.io/blog/tags#perplexity" class="blogging-tag"><code>#perplexity</code></a>

    </div>


</div>

<style>
    .md-typeset .blogging-tags-grid {
        display: flex;
        flex-direction: row;
        flex-wrap: wrap;
        gap: 8px;
        margin-top: 5px;
    }

    .md-typeset .blogging-tag {
        color: var(--md-typeset-color);
        background-color: var(--md-typeset-code-color);
        white-space: nowrap;
        display: block;
    }

    .md-typeset .blogging-tag code {
        border-radius: 5px;
    }
</style>
<h1 id="a-direct-comparison-between-llamacpp-autogptq-exllama-and-transformers-perplexities">A direct comparison between llama.cpp, AutoGPTQ, ExLlama, and transformers perplexities</h1>
<p><strong>Update 1: I added tests with 128g + desc_act using ExLlama. They are marked with (new)</strong></p>
<p><strong>Update 2: also added a test for 30b with 128g + desc_act using ExLlama.</strong></p>
<p><strong>Update 3: the takeaway messages have been updated in light of the latest data.</strong></p>
<p><strong>Update 4: added llama-65b.ggmlv3.q2_K (2-bit) test with llama.cpp.</strong></p>
<hr />
<p>After learning that I could get 1-2 tokens/second for llama-65b on my computer using llama.cpp, I became curious to measure its accuracy. How does it compare to GPTQ?</p>
<p>This led to further questions:</p>
<ul>
<li>ExLlama is a lot faster than AutoGPTQ. Is it as accurate?</li>
<li>How does the <code>load_in_4bit</code> bitsandbytes option compare to all of the previous?</li>
</ul>
<p>The authors of all of those backends take perplexity seriously and have performed their own tests, but I felt like a <em>direct</em> comparison, using not only the same method but also <em>the same code</em>, was lacking. I find this fundamental because small differences in the perplexity evaluation can lead to numbers that are not directly comparable.</p>
<h2 id="how-i-did-it">How I did it</h2>
<p>The idea is to trick the transformers library into thinking that llama.cpp and ExLlama are transformers models, and then evaluate their perplexities.</p>
<p>This is done by creating a wrapper for the model. The first such wrapper was "ExLlama_HF", created by <a href="https://github.com/Larryvrh">LarryVRH</a> in <a href="https://github.com/oobabooga/text-generation-webui/pull/2777">this PR</a>.</p>
<p>What I did was start from Larry's code and </p>
<p>1) Make ExLlama_HF functional for evaluation.</p>
<p>2) Create a llama.cpp_HF wrapper that is also functional for evaluation.</p>
<p>Each of these took more hours to get working than I am willing to admit, but lo and behold, it worked.</p>
<h2 id="evaluation-setup">Evaluation setup</h2>
<p>All tests are performed inside text-generation-webui. It uses the code <a href="https://huggingface.co/docs/transformers/perplexity#calculating-ppl-with-fixedlength-models">here</a>.</p>
<p>The ExLlama tests uses the code in <a href="https://github.com/oobabooga/text-generation-webui/pull/3138">this PR</a>, and the llama.cpp tests use the code in <a href="https://github.com/oobabooga/text-generation-webui/pull/3062">this PR</a>. I haven't merged them yet but they will be in the 1.2 release.</p>
<p>For GPTQ tests, I used models with groupsize 128 and no desc_act, which are the ones that are widely used.</p>
<h2 id="results">Results</h2>
<p>First I will show the results of my personal tests, which are based on the following setup:</p>
<ul>
<li>A .txt input file containing some technical blog posts and papers that I collected. It is a lot smaller and faster to evaluate than wikitext, but I find that it correlates perfectly with bigger evaluations.</li>
<li>Context length of 1200 (otherwise llama-30b-4bit-128g with AutoGPTQ runs out of memory on my RTX 3090).</li>
<li>Stride length of 512.</li>
</ul>
<p>These are the numbers:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Perplexity</th>
<th style="text-align: left;">Backend</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">llama-65b.ggmlv3.q4_K_M.bin</td>
<td style="text-align: right;">4.90639</td>
<td style="text-align: left;">llama.cpp</td>
</tr>
<tr>
<td style="text-align: left;">llama-65b.ggmlv3.q3_K_M.bin</td>
<td style="text-align: right;">5.01299</td>
<td style="text-align: left;">llama.cpp</td>
</tr>
<tr>
<td style="text-align: left;">llama-30b.ggmlv3.q4_K_M.bin</td>
<td style="text-align: right;">5.21557</td>
<td style="text-align: left;">llama.cpp</td>
</tr>
<tr>
<td style="text-align: left;">llama-30b</td>
<td style="text-align: right;">5.24609</td>
<td style="text-align: left;">transformers with <code>--load-in-4bit --use_double_quant</code></td>
</tr>
<tr>
<td style="text-align: left;">Neko-Institute-of-Science_LLaMA-30B-4bit-128g <strong>(new, with desc_act)</strong></td>
<td style="text-align: right;">5.25923</td>
<td style="text-align: left;">ExLlama</td>
</tr>
<tr>
<td style="text-align: left;">llama-30b-4bit-128g</td>
<td style="text-align: right;">5.30078</td>
<td style="text-align: left;">AutoGPTQ</td>
</tr>
<tr>
<td style="text-align: left;">llama-65b.ggmlv3.q2_K.bin</td>
<td style="text-align: right;">5.44745</td>
<td style="text-align: left;">llama.cpp</td>
</tr>
<tr>
<td style="text-align: left;">llama-13b.ggmlv3.q4_K_M.bin</td>
<td style="text-align: right;">5.71705</td>
<td style="text-align: left;">llama.cpp</td>
</tr>
<tr>
<td style="text-align: left;">llama-13b-4bit-128g</td>
<td style="text-align: right;">5.72581</td>
<td style="text-align: left;">ExLlama</td>
</tr>
<tr>
<td style="text-align: left;">llama-13b-4bit-128g</td>
<td style="text-align: right;">5.72656</td>
<td style="text-align: left;">AutoGPTQ</td>
</tr>
<tr>
<td style="text-align: left;">llama-13b</td>
<td style="text-align: right;">5.73047</td>
<td style="text-align: left;">transformers with <code>--load-in-4bit --use_double_quant</code></td>
</tr>
<tr>
<td style="text-align: left;">llama-13b</td>
<td style="text-align: right;">5.73047</td>
<td style="text-align: left;">transformers with <code>--load-in-4bit</code></td>
</tr>
<tr>
<td style="text-align: left;">Neko-Institute-of-Science_LLaMA-13B-4bit-128g <strong>(new, with desc_act)</strong></td>
<td style="text-align: right;">5.74437</td>
<td style="text-align: left;">ExLlama</td>
</tr>
<tr>
<td style="text-align: left;">galactica-30b-4bit-128g</td>
<td style="text-align: right;">6.07812</td>
<td style="text-align: left;">AutoGPTQ</td>
</tr>
<tr>
<td style="text-align: left;">llama-7b</td>
<td style="text-align: right;">6.14453</td>
<td style="text-align: left;">16-bit (no quantization)</td>
</tr>
<tr>
<td style="text-align: left;">facebook_galactica-30b</td>
<td style="text-align: right;">6.16016</td>
<td style="text-align: left;">transformers with <code>--load-in-4bit</code></td>
</tr>
<tr>
<td style="text-align: left;">llama-7b</td>
<td style="text-align: right;">6.24219</td>
<td style="text-align: left;">transformers with <code>--load-in-4bit</code></td>
</tr>
<tr>
<td style="text-align: left;">llama-7b.ggmlv3.q4_K_M.bin</td>
<td style="text-align: right;">6.26391</td>
<td style="text-align: left;">llama.cpp</td>
</tr>
<tr>
<td style="text-align: left;">Neko-Institute-of-Science_LLaMA-7B-4bit-128g <strong>(new, with desc_act)</strong></td>
<td style="text-align: right;">6.28790</td>
<td style="text-align: left;">ExLlama</td>
</tr>
<tr>
<td style="text-align: left;">llama-7b-4bit</td>
<td style="text-align: right;">6.47835</td>
<td style="text-align: left;">ExLlama</td>
</tr>
<tr>
<td style="text-align: left;">llama-7b-4bit</td>
<td style="text-align: right;">6.48438</td>
<td style="text-align: left;">AutoGPTQ</td>
</tr>
<tr>
<td style="text-align: left;">llama-7b-4bit-128g</td>
<td style="text-align: right;">6.54463</td>
<td style="text-align: left;">ExLlama</td>
</tr>
<tr>
<td style="text-align: left;">llama-7b-4bit-128g</td>
<td style="text-align: right;">6.54688</td>
<td style="text-align: left;">AutoGPTQ</td>
</tr>
<tr>
<td style="text-align: left;">facebook_galactica-6.7b</td>
<td style="text-align: right;">6.78906</td>
<td style="text-align: left;">16-bit (no quantization)</td>
</tr>
<tr>
<td style="text-align: left;">tiiuae_falcon-7b</td>
<td style="text-align: right;">7.33203</td>
<td style="text-align: left;">16-bit (no quantization)</td>
</tr>
</tbody>
</table>
<p>As a follow-up, I made a more thorough test with wikitext for llama-13b using 2048 context length and the same 512 stride. This took 2 hours for llama.cpp with all layers offloaded to the GPU. These were the results:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Perplexity</th>
<th style="text-align: left;">Backend</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">llama-13b.ggmlv3.q4_K_M.bin</td>
<td style="text-align: right;">4.58748</td>
<td style="text-align: left;">llama.cpp</td>
</tr>
<tr>
<td style="text-align: left;">Neko-Institute-of-Science_LLaMA-13B-4bit-128g <strong>(new, with desc_act)</strong></td>
<td style="text-align: right;">4.60102</td>
<td style="text-align: left;">ExLlama</td>
</tr>
<tr>
<td style="text-align: left;">llama-13b</td>
<td style="text-align: right;">4.60156</td>
<td style="text-align: left;">transformers with <code>--load-in-4bit</code></td>
</tr>
<tr>
<td style="text-align: left;">llama-13b-4bit-128g</td>
<td style="text-align: right;">4.66016</td>
<td style="text-align: left;">ExLlama</td>
</tr>
<tr>
<td style="text-align: left;">llama-13b-4bit-128g</td>
<td style="text-align: right;">4.66073</td>
<td style="text-align: left;">AutoGPTQ</td>
</tr>
</tbody>
</table>
<h2 id="key-takeaways">Key takeaways</h2>
<ul>
<li>For 13b and 30b, llama.cpp q4_K_M wins.</li>
<li>The perplexity of llama-65b in llama.cpp is indeed lower than for llama-30b in all other backends.</li>
<li>For 7b and 13b, ExLlama is as accurate as AutoGPTQ (a tiny bit lower actually), confirming that its GPTQ reimplementation has been successful. </li>
<li><strong>(updated)</strong> For GPTQ, you should be using models with groupsize AND desc_act on ExLlama unless you have a specific reason to use something else.</li>
<li><strong>(updated)</strong> bitsandbytes <code>load_in_4bit</code> vs GPTQ + desc_act: <code>load_in_4bit</code> wins in 3 out of 4 tests, but the difference is not big.</li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.aecac24b.min.js"></script>
      
    
  </body>
</html>