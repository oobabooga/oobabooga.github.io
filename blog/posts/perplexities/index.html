
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://oobabooga.github.io/blog/posts/perplexities/">
      
      
        <link rel="prev" href="../../tags/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.18">
    
    
      
        <title>A direct comparison between llama.cpp, AutoGPTQ, ExLlama, and transformers perplexities - LLM blog</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.26e3688c.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#a-direct-comparison-between-llamacpp-autogptq-exllama-and-transformers-perplexities" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLM blog" class="md-header__button md-logo" aria-label="LLM blog" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLM blog
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              A direct comparison between llama.cpp, AutoGPTQ, ExLlama, and transformers perplexities
            
          </span>
        </div>
      </div>
    </div>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLM blog" class="md-nav__button md-logo" aria-label="LLM blog" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    LLM blog
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Blog
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../tags/" class="md-nav__link">
        Tags
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Posts
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Posts
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          A direct comparison between llama.cpp, AutoGPTQ, ExLlama, and transformers perplexities
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        A direct comparison between llama.cpp, AutoGPTQ, ExLlama, and transformers perplexities
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#how-i-did-it" class="md-nav__link">
    How I did it
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation-setup" class="md-nav__link">
    Evaluation setup
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    Results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    Key takeaways
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#how-i-did-it" class="md-nav__link">
    How I did it
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation-setup" class="md-nav__link">
    Evaluation setup
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    Results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    Key takeaways
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<div class="md-typeset">
    <div class="blogging-tags-grid">

        <a href="https://oobabooga.github.io/blog/tags#perplexity" class="blogging-tag"><code>#perplexity</code></a>

    </div>


</div>

<style>
    .md-typeset .blogging-tags-grid {
        display: flex;
        flex-direction: row;
        flex-wrap: wrap;
        gap: 8px;
        margin-top: 5px;
    }

    .md-typeset .blogging-tag {
        color: var(--md-typeset-color);
        background-color: var(--md-typeset-code-color);
    }

    .md-typeset .blogging-tag code {
        border-radius: 5px;
    }
</style>
<h1 id="a-direct-comparison-between-llamacpp-autogptq-exllama-and-transformers-perplexities">A direct comparison between llama.cpp, AutoGPTQ, ExLlama, and transformers perplexities</h1>
<p><strong>Update 1: I added tests with 128g + desc_act using ExLlama. They are marked with (new)</strong></p>
<p><strong>Update 2: also added a test for 30b with 128g + desc_act using ExLlama.</strong></p>
<p><strong>Update 3: the takeaway messages have been updated in light of the latest data.</strong></p>
<p>After learning that I could get 1-2 tokens/second for llama-65b on my computer using llama.cpp, I became curious to measure its accuracy. How does it compare to GPTQ?</p>
<p>This led to further questions:</p>
<ul>
<li>ExLlama is a lot faster than AutoGPTQ. Is it as accurate?</li>
<li>How does the <code>load_in_4bit</code> bitsandbytes option compare to all of the previous?</li>
</ul>
<p>The authors of all of those backends take perplexity seriously and have performed their own tests, but I felt like a <em>direct</em> comparison, using not only the same method but also <em>the same code</em>, was lacking. I find this fundamental because small differences in the perplexity evaluation can lead to numbers that are not directly comparable.</p>
<h2 id="how-i-did-it">How I did it</h2>
<p>The idea is to trick the transformers library into thinking that llama.cpp and ExLlama are transformers models, and then evaluate their perplexities.</p>
<p>This is done by creating a wrapper for the model. The first such wrapper was "ExLlama_HF", created by <a href="https://github.com/Larryvrh">LarryVRH</a> in <a href="https://github.com/oobabooga/text-generation-webui/pull/2777">this PR</a>.</p>
<p>What I did was start from Larry's code and </p>
<p>1) Make ExLlama_HF functional for evaluation.</p>
<p>2) Create a llama.cpp_HF wrapper that is also functional for evaluation.</p>
<p>Each of these took more hours to get working than I am willing to admit, but lo and behold, it worked.</p>
<h2 id="evaluation-setup">Evaluation setup</h2>
<p>All tests are performed inside text-generation-webui. It uses the code <a href="https://huggingface.co/docs/transformers/perplexity#calculating-ppl-with-fixedlength-models">here</a>.</p>
<p>The ExLlama tests uses the code in <a href="https://github.com/oobabooga/text-generation-webui/pull/3138">this PR</a>, and the llama.cpp tests use the code in <a href="https://github.com/oobabooga/text-generation-webui/pull/3062">this PR</a>. I haven't merged them yet but they will be in the 1.2 release.</p>
<p>For GPTQ tests, I used models with groupsize 128 and no desc_act, which are the ones that are widely used.</p>
<h2 id="results">Results</h2>
<p>First I will show the results of my personal tests, which are based on the following setup:</p>
<ul>
<li>A .txt input file containing some technical blog posts and papers that I collected. It is a lot smaller and faster to evaluate than wikitext, but I find that it correlates perfectly with bigger evaluations.</li>
<li>Context length of 1200 (otherwise llama-30b-4bit-128g with AutoGPTQ runs out of memory on my RTX 3090).</li>
<li>Stride length of 512.</li>
</ul>
<p>These are the numbers:</p>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="right">Perplexity</th>
<th align="left">Backend</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">llama-65b.ggmlv3.q4_K_M.bin</td>
<td align="right">4.90639</td>
<td align="left">llama.cpp</td>
</tr>
<tr>
<td align="left">llama-65b.ggmlv3.q3_K_M.bin</td>
<td align="right">5.01299</td>
<td align="left">llama.cpp</td>
</tr>
<tr>
<td align="left">llama-30b.ggmlv3.q4_K_M.bin</td>
<td align="right">5.21557</td>
<td align="left">llama.cpp</td>
</tr>
<tr>
<td align="left">llama-30b</td>
<td align="right">5.24609</td>
<td align="left">transformers with <code>--load-in-4bit --use_double_quant</code></td>
</tr>
<tr>
<td align="left">Neko-Institute-of-Science_LLaMA-30B-4bit-128g <strong>(new, with desc_act)</strong></td>
<td align="right">5.25923</td>
<td align="left">ExLlama</td>
</tr>
<tr>
<td align="left">llama-30b-4bit-128g</td>
<td align="right">5.30078</td>
<td align="left">AutoGPTQ</td>
</tr>
<tr>
<td align="left">llama-13b.ggmlv3.q4_K_M.bin</td>
<td align="right">5.71705</td>
<td align="left">llama.cpp</td>
</tr>
<tr>
<td align="left">llama-13b-4bit-128g</td>
<td align="right">5.72581</td>
<td align="left">ExLlama</td>
</tr>
<tr>
<td align="left">llama-13b-4bit-128g</td>
<td align="right">5.72656</td>
<td align="left">AutoGPTQ</td>
</tr>
<tr>
<td align="left">llama-13b</td>
<td align="right">5.73047</td>
<td align="left">transformers with <code>--load-in-4bit --use_double_quant</code></td>
</tr>
<tr>
<td align="left">llama-13b</td>
<td align="right">5.73047</td>
<td align="left">transformers with <code>--load-in-4bit</code></td>
</tr>
<tr>
<td align="left">Neko-Institute-of-Science_LLaMA-13B-4bit-128g <strong>(new, with desc_act)</strong></td>
<td align="right">5.74437</td>
<td align="left">ExLlama</td>
</tr>
<tr>
<td align="left">galactica-30b-4bit-128g</td>
<td align="right">6.07812</td>
<td align="left">AutoGPTQ</td>
</tr>
<tr>
<td align="left">llama-7b</td>
<td align="right">6.14453</td>
<td align="left">16-bit (no quantization)</td>
</tr>
<tr>
<td align="left">facebook_galactica-30b</td>
<td align="right">6.16016</td>
<td align="left">transformers with <code>--load-in-4bit</code></td>
</tr>
<tr>
<td align="left">llama-7b</td>
<td align="right">6.24219</td>
<td align="left">transformers with <code>--load-in-4bit</code></td>
</tr>
<tr>
<td align="left">llama-7b.ggmlv3.q4_K_M.bin</td>
<td align="right">6.26391</td>
<td align="left">llama.cpp</td>
</tr>
<tr>
<td align="left">Neko-Institute-of-Science_LLaMA-7B-4bit-128g <strong>(new, with desc_act)</strong></td>
<td align="right">6.28790</td>
<td align="left">ExLlama</td>
</tr>
<tr>
<td align="left">llama-7b-4bit</td>
<td align="right">6.47835</td>
<td align="left">ExLlama</td>
</tr>
<tr>
<td align="left">llama-7b-4bit</td>
<td align="right">6.48438</td>
<td align="left">AutoGPTQ</td>
</tr>
<tr>
<td align="left">llama-7b-4bit-128g</td>
<td align="right">6.54463</td>
<td align="left">ExLlama</td>
</tr>
<tr>
<td align="left">llama-7b-4bit-128g</td>
<td align="right">6.54688</td>
<td align="left">AutoGPTQ</td>
</tr>
<tr>
<td align="left">facebook_galactica-6.7b</td>
<td align="right">6.78906</td>
<td align="left">16-bit (no quantization)</td>
</tr>
<tr>
<td align="left">tiiuae_falcon-7b</td>
<td align="right">7.33203</td>
<td align="left">16-bit (no quantization)</td>
</tr>
</tbody>
</table>
<p>As a follow-up, I made a more thorough test with wikitext for llama-13b using 2048 context length and the same 512 stride. This took 2 hours for llama.cpp with all layers offloaded to the GPU. These were the results:</p>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="right">Perplexity</th>
<th align="left">Backend</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">llama-13b.ggmlv3.q4_K_M.bin</td>
<td align="right">4.58748</td>
<td align="left">llama.cpp</td>
</tr>
<tr>
<td align="left">Neko-Institute-of-Science_LLaMA-13B-4bit-128g <strong>(new, with desc_act)</strong></td>
<td align="right">4.601023</td>
<td align="left">ExLlama</td>
</tr>
<tr>
<td align="left">llama-13b</td>
<td align="right">4.60156</td>
<td align="left">transformers with <code>--load-in-4bit</code></td>
</tr>
<tr>
<td align="left">llama-13b-4bit-128g</td>
<td align="right">4.66016</td>
<td align="left">ExLlama</td>
</tr>
<tr>
<td align="left">llama-13b-4bit-128g</td>
<td align="right">4.66073</td>
<td align="left">AutoGPTQ</td>
</tr>
</tbody>
</table>
<h2 id="key-takeaways">Key takeaways</h2>
<ul>
<li>For 13b and 30b, llama.cpp q4_K_M wins.</li>
<li>The perplexity of llama-65b in llama.cpp is indeed lower than for llama-30b in all other backends.</li>
<li>For 7b and 13b, ExLlama is as accurate as AutoGPTQ (a tiny bit lower actually), confirming that its GPTQ reimplementation has been successful. </li>
<li><strong>(updated)</strong> For GPTQ, you should be using models with groupsize AND desc_act on ExLlama unless you have a specific reason to use something else.</li>
<li><strong>(updated)</strong> bitsandbytes <code>load_in_4bit</code> vs GPTQ + desc_act: <code>load_in_4bit</code> wins in 3 out of 4 tests, but the difference is not big.</li>
</ul>
<hr />
<div style="background-color: #f2f2f2; border-radius: 10px; padding: 30px; width: 400px; text-align: center; margin-left: auto; margin-right: auto; margin-top: 50px">
  <h2 style="font-size: 24px; margin-bottom: 20px; margin-top:20px">Support My Work</h2>
  <p style="font-size: 18px; margin-bottom: 30px;">If you appreciate what I do, consider supporting me:</p>
  <div style="text-align:center">
<script type='text/javascript' src='https://storage.ko-fi.com/cdn/widget/Widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Support Me on Ko-fi', '#29abe0', 'B0B5JFPBO');kofiwidget2.draw();</script> 
  </div>
</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.220ee61c.min.js"></script>
      
    
  </body>
</html>